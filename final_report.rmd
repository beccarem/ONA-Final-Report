---
title: "Final Report"
author: "Chelsea, Rebecca, Diwei, Dany"
output:
  github_document: default
  pdf_document: default
  fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(arrow)
```

# 1. Introduction

The U.S. Patent and Trademark Office (USPTO) is the agency within the U.S. Chamber of Commerce that issues patents and trademarks. It is one in few organizations around the world to guarantee the intellectual property rights or inventors. Although the organization plays an important role in the capitalist society, it is now faced with many challenges and criticisms that it needs to address. 

One of the challenges is the time it takes examiners to process an application. As the agency is fully funded by patent applications fees, the focus should be to ensure a satisfactory "customer experience." However, when examiners take a long time to process applications, this can be a frustrating experience for applicants. USPTO is now taking action to improve its entire process by identifying the causes of application backlogs.

Our goal is to understand what causes delays in processing times, which characteristics makes examiners more efficient in their work and how can network analysis solve the organizational difficulties that USPTO is facing. We will focus on determining the organizational and social factors associated with the length of patent application prosecution. We will look at how examiner's demographics (gender, race, tenure) are related to their application processing time. We will also look at the social advice network within the organization plays a role in improving the patent application times and outcomes.

# 2. Methodology
1. Data Pre-processing
a. Data Engineering
b. Data Exploration
c. Data Cleaning
2. Examining Gender Effect
3. Examining Race Effect
4. Examining Tenure Effect

# 3. Analysis and Results
Load the applications data from 'app_data_sample.parquet' and edges data from 'edges_sample.csv'

```{r load-data, include=FALSE}
applications <- read_parquet("app_data_sample.parquet")
edges <- read_csv("edges_sample.csv")
```

### Get gender for examiners

We'll get gender based on the first name of the examiner, which is recorded in the field `examiner_name_first`. We'll use library `gender` for that, relying on a modified version of their own [example](https://cran.r-project.org/web/packages/gender/vignettes/predicting-gender.html).

Note that there are over 2 million records in the applications table -- that's because there are many records for each examiner, as many as the number of applications that examiner worked on during this time frame. Our first step therefore is to get all *unique* names in a separate list `examiner_names`. We will then guess gender for each one and will join this table back to the original dataset. So, let's get names without repetition:

```{r gender-1, include=FALSE}
library(gender)
#install_genderdata_package() # only run this line the first time you use the package, to get data for it
# get a list of first names without repetitions
examiner_names <- applications %>% 
  distinct(examiner_name_first)
#examiner_names
```

Now let's use function `gender()` as shown in the example for the package to attach a gender and probability to each name and put the results into the table `examiner_names_gender`

```{r gender-2, echo=FALSE}
# get a table of names and gender
examiner_names_gender <- examiner_names %>% 
  do(results = gender(.$examiner_name_first, method = "ssa")) %>% 
  unnest(cols = c(results), keep_empty = TRUE) %>% 
  select(
    examiner_name_first = name,
    gender,
    proportion_female
  )
examiner_names_gender
```

Finally, let's join that table back to our original applications data and discard the temporary tables we have just created to reduce clutter in our environment.

```{r gender-3, include=FALSE}
# remove extra colums from the gender table
examiner_names_gender <- examiner_names_gender %>% 
  select(examiner_name_first, gender)
# joining gender back to the dataset
applications <- applications %>% 
  left_join(examiner_names_gender, by = "examiner_name_first")
# cleaning up
#gc()
```

### Guess the examiner's race

We'll now use package `wru` to estimate likely race of an examiner. Just like with gender, we'll get a list of unique names first, only now we are using surnames.

```{r race-1, include=FALSE}
library(wru)
examiner_surnames <- applications %>% 
  select(surname = examiner_name_last) %>% 
  distinct()
#examiner_surnames
```

We'll follow the instructions for the package outlined here [https://github.com/kosukeimai/wru](https://github.com/kosukeimai/wru).

```{r race-2, include=FALSE}
examiner_race <- predict_race(voter.file = examiner_surnames, surname.only = T) %>% 
  as_tibble()
#examiner_race
```

As you can see, we get probabilities across five broad US Census categories: white, black, Hispanic, Asian and other. (Some of you may correctly point out that Hispanic is not a race category in the US Census, but these are the limitations of this package.)

Our final step here is to pick the race category that has the highest probability for each last name and then join the table back to the main applications table. See this example for comparing values across columns: [https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-rowwise/](https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-rowwise/). And this one for `case_when()` function: [https://dplyr.tidyverse.org/reference/case_when.html](https://dplyr.tidyverse.org/reference/case_when.html).

```{r race-3, echo=FALSE}
examiner_race <- examiner_race %>% 
  mutate(max_race_p = pmax(pred.asi, pred.bla, pred.his, pred.oth, pred.whi)) %>% 
  mutate(race = case_when(
    max_race_p == pred.asi ~ "asian",
    max_race_p == pred.bla ~ "black",
    max_race_p == pred.his ~ "hispanic",
    max_race_p == pred.oth ~ "other",
    max_race_p == pred.whi ~ "white",
    TRUE ~ NA_character_
  ))
examiner_race
```

Let's join the data back to the applications table.

```{r race-4, include=FALSE}
# removing extra columns
examiner_race <- examiner_race %>% 
  select(surname,race)
applications <- applications %>% 
  left_join(examiner_race, by = c("examiner_name_last" = "surname"))
#gc()
```

### Examiner's tenure 

To figure out the timespan for which we observe each examiner in the applications data, let's find the first and the last observed date for each examiner. We'll first get examiner IDs and application dates in a separate table, for ease of manipulation. We'll keep examiner ID (the field `examiner_id`), and earliest and latest dates for each application (`filing_date` and `appl_status_date` respectively). We'll use functions in package `lubridate` to work with date and time values.

```{r tenure-1, include=FALSE}
library(lubridate) # to work with dates
examiner_dates <- applications %>% 
  select(examiner_id, filing_date, appl_status_date) 
#examiner_dates
```

The dates look inconsistent in terms of formatting. Let's make them consistent. We'll create new variables `start_date` and `end_date`.

```{r tenure-2, include=FALSE}
examiner_dates <- examiner_dates %>% 
  mutate(start_date = ymd(filing_date), end_date = as_date(dmy_hms(appl_status_date)))
```

Let's now identify the earliest and the latest date for each examiner and calculate the difference in days, which is their tenure in the organization.

```{r tenure-3, echo=FALSE}
examiner_dates <- examiner_dates %>% 
  group_by(examiner_id) %>% 
  summarise(
    earliest_date = min(start_date, na.rm = TRUE), 
    latest_date = max(end_date, na.rm = TRUE),
    tenure_days = interval(earliest_date, latest_date) %/% days(1)
    ) %>% 
  filter(year(latest_date)<2018)
#examiner_dates

## plot tenure
library(skimr)
examiner_dates %>% 
  select(earliest_date,latest_date,tenure_days) %>% 
  skim()
```

Joining back to the applications data.

```{r tenure-4, include=FALSE}
applications <- applications %>% 
  left_join(examiner_dates, by = "examiner_id")
#rm(examiner_dates)
#gc()
```

### Create application processing time variable
Compute the final decision date as either abandon date or patent issue date.
```{r, echo=FALSE}
attach(applications)
library(lubridate)

# compute the final decision date as either abandon date or patent issue date
application_dates <- applications %>% 
    mutate(decision_date = coalesce(abandon_date,patent_issue_date)) %>%
    select(application_number,filing_date, abandon_date, patent_issue_date, decision_date, examiner_id, examiner_art_unit, gender, race, tenure_days) %>%
    filter(!is.na(decision_date))

head(application_dates)
```

```{r, echo=FALSE}
# compute the application processing time as the difference of filing date and decision date
application_dates <- application_dates %>% 
    #mutate(app_proc_time = decision_date - filing_date)
    mutate(app_proc_time = difftime(decision_date, filing_date, units = "days"))

head(application_dates) #1,688,716 applications
```

It seems some application processing time have negative value abnormally. Let's take a look at the distribution.

```{r, echo=FALSE}
# plot the data distribution of application processing time
application_dates %>%
  ggplot(aes(sample = as.numeric(app_proc_time))) +
  geom_qq()
```

```{r, include=FALSE}
# filter out negative and outlying application processing time
application_dates <- application_dates %>% 
    filter(app_proc_time>ddays(0)) %>% 
    filter(app_proc_time<ddays(10000))

head(application_dates) #1,688,672 applications
```
```{r, echo=FALSE}
# plot again the data distribution of application processing time after cleaning
application_dates %>%
  ggplot(aes(sample = as.numeric(app_proc_time))) +
  geom_qq()
```
Filter out negative and outlying application processing time. Outliers are removed successfully. 

### Get work group from art unit
```{r, include=FALSE}
library(plyr)
library(dplyr)
library(lubridate)
```


```{r, include=FALSE}
# before we begin, get the workgroup from art unit as rounding down to digit tenth.
application_dates <- application_dates %>%
  mutate(wg = (application_dates$examiner_art_unit%/%10) * 10)

# Find out which is the dominating workgroup an examiner handled the applications for.
application_dates <- mutate(
  application_dates,
  period = case_when(
    filing_date<ymd("2007-01-01") ~ NA_character_,
    filing_date<ymd("2008-01-01") ~ "t0",
    filing_date<ymd("2009-01-01") ~ "t1",
    filing_date<ymd("2010-01-01") ~ "t2",
    filing_date<ymd("2011-01-01") ~ "t3",
    filing_date<ymd("2012-01-01") ~ "t4",
    filing_date<ymd("2013-01-01") ~ "t5",
    filing_date<ymd("2014-01-01") ~ "t6",
    filing_date<ymd("2015-01-01") ~ "t7",
    filing_date<ymd("2016-01-01") ~ "t8",
    TRUE~ NA_character_)
  )

# get number of applications
library(plyr)
examiner_wg_napp <- ddply(application_dates, .(examiner_id, period, wg), nrow)
names(examiner_wg_napp) <- c("examiner_id","period", "wg", "n_applications")

# assume an examiner belong to the wg he/she most frequently handled applications for, if tie take the greater wg
examiner_wg_napp <- examiner_wg_napp[order(examiner_wg_napp$examiner_id, examiner_wg_napp$period, -(examiner_wg_napp$n_applications), -(examiner_wg_napp$wg)), ] ### sort first
examiner_wg <- examiner_wg_napp [!duplicated(examiner_wg_napp[c(1,2)]),]
examiner_wg <- select(examiner_wg, c("examiner_id","wg","period"))
examiner_wg <- drop_na(examiner_wg)

```

### Get seniority at each period
Let's assume a time period of $t_0 = 2007$ (the year we first get senior examiners, according to our definition), $t_1 = 2008$, $t_2 = 2009$, $t_3 = 2010$, $t_4 = 2011$, $t_5 = 2012$, $t_6 = 2013$, $t_7 = 2014$, $t_8 = 2015$

```{r, include=FALSE}
library(ggplot2)
library(scales)  
library(gridExtra)
```


```{r, echo=FALSE}
# Get tenure & state at each period
examiner_dates <- examiner_dates %>% 
  mutate(
    tenure_t0 = ifelse(as.duration(earliest_date %--% ymd("2007-01-01")) / dyears(1)<0,0,as.duration(earliest_date %--% ymd("2007-01-01"))/dyears(1)),
    tenure_t1 = ifelse(as.duration(earliest_date %--% ymd("2008-01-01")) / dyears(1)<0,0,as.duration(earliest_date %--% ymd("2008-01-01"))/dyears(1)),
    tenure_t2 = ifelse(as.duration(earliest_date %--% ymd("2009-01-01")) / dyears(1)<0,0,as.duration(earliest_date %--% ymd("2009-01-01"))/dyears(1)),
    tenure_t3 = ifelse(as.duration(earliest_date %--% ymd("2010-01-01")) / dyears(1)<0,0,as.duration(earliest_date %--% ymd("2010-01-01"))/dyears(1)),
    tenure_t4 = ifelse(as.duration(earliest_date %--% ymd("2011-01-01")) / dyears(1)<0,0,as.duration(earliest_date %--% ymd("2011-01-01"))/dyears(1)),
    tenure_t5 = ifelse(as.duration(earliest_date %--% ymd("2012-01-01")) / dyears(1)<0,0,as.duration(earliest_date %--% ymd("2012-01-01"))/dyears(1)),
    tenure_t6 = ifelse(as.duration(earliest_date %--% ymd("2013-01-01")) / dyears(1)<0,0,as.duration(earliest_date %--% ymd("2013-01-01"))/dyears(1)),
    tenure_t7 = ifelse(as.duration(earliest_date %--% ymd("2014-01-01")) / dyears(1)<0,0,as.duration(earliest_date %--% ymd("2014-01-01"))/dyears(1)),
    tenure_t8 = ifelse(as.duration(earliest_date %--% ymd("2015-01-01")) / dyears(1)<0,0,as.duration(earliest_date %--% ymd("2015-01-01"))/dyears(1)),

    t0_state = case_when(
      tenure_t0<6 & tenure_t0>0 ~ "Junior"  , # Jr; not those yet to be hired!
      tenure_t0>=6              ~ "Senior"  , # Sr
      TRUE                      ~ NA_character_ # not yet hired
    ),
    t1_state = case_when(
      latest_date<ymd("2008-12-31")        ~ "Exit",
      earliest_date>ymd("2007-01-01") 
        & earliest_date<ymd("2008-01-01") ~ "New hire",
      tenure_t1<6 & tenure_t1>0          ~ "Junior"  , # Jr; not those yet to be hired!
      tenure_t1>=6                       ~ "Senior"  , # Sr
      TRUE                               ~ NA_character_ # not yet hired
      ),
    t2_state = case_when(
      t1_state=="Exit"                   ~ NA_character_,
      latest_date<ymd("2009-12-31")        ~ "Exit",
      earliest_date>ymd("2008-01-01") 
        & earliest_date<ymd("2009-01-01") ~ "New hire",
      tenure_t2<6 & tenure_t2>0          ~ "Junior"  , # Jr; not those yet to be hired!
      tenure_t2>=6                       ~ "Senior"  , # Sr
      TRUE                               ~ NA_character_ # not yet hired or already exit in previous period
      ),
    t3_state = case_when(
      t1_state=="Exit"|t2_state=="Exit"                   ~ NA_character_,
      latest_date<ymd("2010-12-31")        ~ "Exit",
      earliest_date>ymd("2009-01-01") 
        & earliest_date<ymd("2010-01-01") ~ "New hire",
      tenure_t3<6 & tenure_t3>0          ~ "Junior"  , # Jr; not those yet to be hired!
      tenure_t3>=6                       ~ "Senior"  , # Sr
      TRUE                               ~ NA_character_ # not yet hired or already exit in previous period
      ),
    t4_state = case_when(
      t1_state=="Exit"|t2_state=="Exit"|t3_state=="Exit"                   ~ NA_character_,
      latest_date<ymd("2011-12-31")        ~ "Exit",
      earliest_date>ymd("2010-01-01") 
        & earliest_date<ymd("2011-01-01") ~ "New hire",
      tenure_t4<6 & tenure_t4>0          ~ "Junior"  , # Jr; not those yet to be hired!
      tenure_t4>=6                       ~ "Senior"  , # Sr
      TRUE                               ~ NA_character_ # not yet hired or already exit in previous period
      ),
    t5_state = case_when(
      t1_state=="Exit"|t2_state=="Exit"|t3_state=="Exit"|t4_state=="Exit"                   ~ NA_character_,
      latest_date<ymd("2012-12-31")        ~ "Exit",
      earliest_date>ymd("2011-01-01") 
        & earliest_date<ymd("2012-01-01") ~ "New hire",
      tenure_t5<6 & tenure_t5>0          ~ "Junior"  , # Jr; not those yet to be hired!
      tenure_t5>=6                       ~ "Senior"  , # Sr
      TRUE                               ~ NA_character_ # not yet hired or already exit in previous period
      ),
    t6_state = case_when(
      t1_state=="Exit"|t2_state=="Exit"|t3_state=="Exit"|t4_state=="Exit"|t5_state=="Exit"                   ~ NA_character_,
      latest_date<ymd("2013-12-31")        ~ "Exit",
      earliest_date>ymd("2012-01-01") 
        & earliest_date<ymd("2013-01-01") ~ "New hire",
      tenure_t6<6 & tenure_t6>0          ~ "Junior"  , # Jr; not those yet to be hired!
      tenure_t6>=6                       ~ "Senior"  , # Sr
      TRUE                               ~ NA_character_ # not yet hired or already exit in previous period
      ),
    t7_state = case_when(
      t1_state=="Exit"|t2_state=="Exit"|t3_state=="Exit"|t4_state=="Exit"|t5_state=="Exit"|t6_state=="Exit"                   ~ NA_character_,
      latest_date<ymd("2014-12-31")        ~ "Exit",
      earliest_date>ymd("2013-01-01") 
        & earliest_date<ymd("2014-01-01") ~ "New hire",
      tenure_t7<6 & tenure_t7>0          ~ "Junior"  , # Jr; not those yet to be hired!
      tenure_t7>=6                       ~ "Senior"  , # Sr
      TRUE                               ~ NA_character_ # not yet hired or already exit in previous period
      ),
    t8_state = case_when(
      t1_state=="Exit"|t2_state=="Exit"|t3_state=="Exit"|t4_state=="Exit"|t5_state=="Exit"|t6_state=="Exit"|t7_state=="Exit"                   ~ NA_character_,
      latest_date<ymd("2015-12-31")        ~ "Exit",
      earliest_date>ymd("2014-01-01") 
        & earliest_date<ymd("2015-01-01") ~ "New hire",
      tenure_t8<6 & tenure_t8>0          ~ "Junior"  , # Jr; not those yet to be hired!
      tenure_t8>=6                       ~ "Senior"  , # Sr
      TRUE                               ~ NA_character_ # not yet hired or already exit in previous period
      )
    )

examiner_dates <- examiner_dates %>% 
  select(examiner_id, t0_state, t1_state, t2_state, t3_state, t4_state, t5_state, t6_state, t7_state, t8_state)

## plot seniority
plot1 <- ggplot(examiner_dates, aes(factor(t1_state, levels = c("New hire", "Junior", "Senior", "Exit", NA)))) + 
          geom_bar(aes(y = (..count..)/sum(..count..))) + 
          scale_y_continuous(labels=scales::percent) +
          ylab("Relative Frequencies") +
          ggtitle("Seniority distribution for USPTO at t1")
plot1
```

Joining back to the applications dates data.

```{r, include=FALSE}
application_dates <- application_dates %>% 
  left_join(examiner_dates, by = "examiner_id")
#gc()
```

### Generate examiner panel dataset
Compute average application processing time
```{r, echo=FALSE}
# compute average application processing time

cols <- c("examiner_id","period", "wg", "examiner_art_unit","gender", "race", "tenure_days",
          "t0_state","t1_state","t2_state","t3_state","t4_state","t5_state","t6_state","t7_state","t8_state")

examiners <- application_dates %>%
    group_by(across(all_of(cols))) %>%
    dplyr::summarize(mean_app_proc_time = mean(app_proc_time, na.rm=TRUE), n_app = n()) %>%
    drop_na()

head(data.frame(examiners))
```

#### Create panel dataset of examiners for analysis
```{r, echo=FALSE}
# subset examiners for time period t1 for our analysis, as advice dates are all in 2008 
examiner_aus <- data.frame(examiners) %>%
    filter(period == "t1") %>% 
    #filter(wg == 2450 | wg == 2480) %>%
    select(wg, examiner_art_unit, examiner_id, gender, race, t1_state, tenure_days, mean_app_proc_time, n_app) %>%
    distinct(examiner_id, .keep_all=TRUE) %>% 
    drop_na() 

head(examiner_aus) #2591
```

### Compute centrality of examiners
```{r, include=FALSE}
# separate from edges examiners seek and give advice
edges_aus <- edges %>%
  filter(ego_examiner_id %in% examiner_aus$examiner_id) %>%
  filter(alter_examiner_id %in% examiner_aus$examiner_id) %>%
  drop_na() #8824
```

```{r, include=FALSE}
# merge work group information
network <- left_join(edges_aus, examiner_aus, by = c("ego_examiner_id" = "examiner_id"))
colnames(network)[5] <- "ego_examiner_wg"
colnames(network)[6] <- "ego_examiner_au"
colnames(network)[7] <- "ego_examiner_gender"
colnames(network)[8] <- "ego_examiner_race"
colnames(network)[9] <- "ego_examiner_t1_state"
colnames(network)[10] <- "ego_examiner_tenure"
colnames(network)[11] <- "ego_examiner_appprooctime"
colnames(network)[12] <- "ego_examiner_napp"
#network <- subset(network, select = -c(period))
network <- left_join(network, examiner_aus, by = c("alter_examiner_id" = "examiner_id"))
colnames(network)[13] <- "alter_examiner_wg"
colnames(network)[14] <- "alter_examiner_au"
colnames(network)[15] <- "alter_examiner_gender"
colnames(network)[16] <- "alter_examiner_race"
colnames(network)[17] <- "alter_examiner_t1_state"
#colnames(network)[18] <- "alter_examiner_tenure"
colnames(network)[19] <- "alter_examiner_appprooctime"
colnames(network)[20] <- "alter_examiner_napp"
#network <- subset(network, select = -c(period))

head(network)
```


```{r, echo=FALSE}
# Visualize the advice seeking volume by examiner seniority in period t1
  network %>% 
  group_by(ego_examiner_t1_state, alter_examiner_t1_state) %>% 
  dplyr::summarise(count = n())
```
There are more junior seeking advice from senior than peer advice seeking (junior to junior, senior to senior). It is the fewest for senior to seek advice from junior. 

```{r, include=FALSE}
# create edge list
edge_list <- select(network, c("ego_examiner_id","alter_examiner_id")) #8824

# create node list
ego <- select(network, c("ego_examiner_id","ego_examiner_wg")) %>%
    dplyr::rename(id=ego_examiner_id, wg=ego_examiner_wg)
alter <- select(network, c("alter_examiner_id","alter_examiner_wg")) %>%
    dplyr::rename(id=alter_examiner_id, wg=alter_examiner_wg)
nodes <- rbind(ego, alter) %>%
  select(id) %>%
  distinct() %>%
  drop_na() #1447

# create advice net
library(igraph)
advice_net = graph_from_data_frame(d=edge_list, vertices=nodes, directed=TRUE)
```

```{r, include=FALSE}
# calculate Degree Centrality, a measure for a node in a network is just its degree, the number of edges connected to it. 
V(advice_net)$dc <- degree(advice_net)
# calculate Betweenness Centrality, which measures the extent to which a node lies on paths between other nodes.
V(advice_net)$bc <- betweenness(advice_net)
# calculate Eigenvector Centrality, which awards a number of points proportional to the centrality scores of the neighbors
V(advice_net)$ec <- evcent(advice_net)$vector
V(advice_net)$cc <- closeness(advice_net) # dropped since closeness centrality is not well-defined for disconnected graphs
```

```{r, include=FALSE}
# combine the centrality scores
centrality <- data.frame(cbind(nodes$id, V(advice_net)$dc, V(advice_net)$bc, V(advice_net)$ec, V(advice_net)$cc)) 
colnames(centrality)[1] <- "examiner_id"
colnames(centrality)[2] <- "degree_centrality"
colnames(centrality)[3] <- "betweenness_centrality"
colnames(centrality)[4] <- "eigenvector_centrality"
colnames(centrality)[5] <- "closeness_centrality"
head(centrality)
```

```{r, include=FALSE}
# merge centrality to examiners
examiner_joined <- left_join(examiner_aus, centrality, by = c("examiner_id" = "examiner_id"))
examiner_joined <- examiner_joined %>%
  drop_na(degree_centrality)
head(examiner_joined) #1447
```

### Work groups selection (applicable for analysis zoom-in)
```{r, echo=FALSE}
# select the workgroups under the same technology centre with most examiners at t1 for our analysis, as advice dates are all in 2008
examiner_joined %>% 
  #dplyr::filter(period == "t1") %>% 
  count("wg") %>% 
  arrange(desc(freq)) %>%
  head(4)
```

Hence, we're selecting work groups 1780 and 1770 under the same technology centre 1700 for further analysis. 
```{r, include=FALSE}
examiner_joined_1780 = examiner_joined[examiner_joined$wg==1780,]
examiner_joined_1770 = examiner_joined[examiner_joined$wg==1770,]

examiner_joined_2wg <- examiner_joined %>%
  filter(wg == 1780 | wg == 1770)
```

### 3.1 Organization and social factors associated with the length of patent application prosecution
Before we begin, let's take a look at the distribution of application processing time for the selected examiners.
```{r, echo=FALSE}
# plot the data distribution of application processing time
plot1 <- examiner_joined %>% ggplot(aes(sample = as.numeric(mean_app_proc_time))) + geom_qq() + labs(title="App Proc Time for USPTO") + ylim(0,3500)
plot2 <- examiner_joined_2wg %>% ggplot(aes(sample = as.numeric(mean_app_proc_time))) + geom_qq() + labs(title="App Proc Time for 2wg") + ylim(0,3500)
plot3 <- examiner_joined_1780 %>% ggplot(aes(sample = as.numeric(mean_app_proc_time))) + geom_qq() + labs(title="App Proc Time for 1780") + ylim(0,3500)
plot4 <- examiner_joined_1770 %>% ggplot(aes(sample = as.numeric(mean_app_proc_time))) + geom_qq() + labs(title="App Proc Time for 1770") + ylim(0,3500)
grid.arrange(plot1,plot2,plot3,plot4, ncol=2, widths=c(1,1))
```

We can see that the two selected work groups under the same technology centre have lower application processing time than USTPO company as a whole. Comparatively, 1780 demonstrates more similar distribution as company-wide, while work group 1770 has even shorter application proc time in general but slightly more outliers towards the higher end. Given the low sample size (79 examiners for 1780 and 66 examiners for 1770), the data limitation shall be acknowledged. 

Also we would like to understand the network for the two selected work groups. 

```{r, echo=FALSE}
nodes <- left_join(nodes, examiner_joined, by = c("id" = "examiner_id")) %>% select(id,wg,examiner_art_unit)
nodes_2wg <- nodes %>% filter(wg == 1780 | wg == 1770)
edge_list_2wg <- edge_list %>% filter(ego_examiner_id %in% nodes_2wg$id) %>% filter(alter_examiner_id %in% nodes_2wg$id)
advice_net_2wg = graph_from_data_frame(d=edge_list_2wg, vertices=nodes_2wg, directed=TRUE)

V(advice_net_2wg)$dc <- degree(advice_net_2wg)

library(ggraph)
ggraph(advice_net_2wg, layout="kk") +
  geom_edge_link()+
  geom_node_point(aes(size=dc, color=nodes_2wg$examiner_art_unit), show.legend=T)
```

#### 3.1.1 Impacts of centrality
As a consulting team, we would like to analyze how examiners' gender is associated with the length of the patent application processing time. Here we would focus on both the USPTO organizational level and selected work groups levels in t1 period. 

Firstly, we run 4 linear regression models on organizational level to understand the effect of each centrality measure to the application process time separately.

```{r, echo=FALSE}
#install.packages("stargazer")
library(stargazer)

reg1 = lm(as.numeric(mean_app_proc_time)~degree_centrality,data=examiner_joined)
reg2 = lm(as.numeric(mean_app_proc_time)~betweenness_centrality,data=examiner_joined)
reg3 = lm(as.numeric(mean_app_proc_time)~eigenvector_centrality,data=examiner_joined)
reg4 = lm(as.numeric(mean_app_proc_time)~closeness_centrality,data=examiner_joined)

stargazer(reg1,reg2,reg3,reg4,type="text", title="Impacts of Each Centrality Measure to Application Processing Time (USPTO level)")
```

Both degree centrality and betweenness centrality have a negative relation to the mean application processing time. Adding one more unit in degree centrality and betweenness centrality subtract, on average, mean application processing time by 0.101 days and 0.013 days respectively, if holding everything else equal. The higher these centrality scores, the faster the application processing. 

Both eigenvector centrality and closesness centrality (ranged from 0 to 1 ) have a positive relation to the mean application processing time. Adding 0.1 more unit in eigenvector centrality adds mean application processing time by 27.2 days and adding 0.1 more unit in eigenvector centrality adds mean application processing time by 1.3e08 days, if holding everything else equal. The higher these centrality scores, the slower the application processing. This means having relationship with examiners who have high scores would take longer processing time, potentially due to more workload assigned, and the more distant an examiner is with other examiners, the more the longer the processing time, potentially due to lack of peer support. 

Now, let's take a look at the correlation of the 4 centrality measures and run a regression to understand the effect of the centrality measure to the application process time together. 

```{r, echo=FALSE}
library(ggcorrplot)
quantvars <- examiner_joined %>% select(mean_app_proc_time, tenure_days, n_app, degree_centrality, betweenness_centrality, eigenvector_centrality, closeness_centrality)
quantvars$mean_app_proc_time = as.numeric(quantvars$mean_app_proc_time)
# populating correlation matrix
corr_matrix = cor(quantvars)
ggcorrplot(corr_matrix)
```
From the correlation matrix we can see that the target variable app_proc_time has no strong correlation with other numeric variables. The centrality measures have strong correlation with each other - relatively stronger for degree centrality with all other measures and slightly stronger correlation between betweenness centrality and closeness centrality. 

Then, we look into all measures in one linear regression model and observed consistent results - an increase in degree centrality and betweenness centrality reduces application processing time while an in crease in eigenvector centrality and closeness centrality increases application processing time. 

```{r, echo=FALSE}
reg5 = lm(as.numeric(mean_app_proc_time)~degree_centrality+betweenness_centrality+eigenvector_centrality+closeness_centrality,data=examiner_joined)
stargazer(reg1,reg2,reg3,reg4,reg5,type="text", title="Impacts of Centrality Measure Separately and Combined to Application Processing Time (USPTO level)")
```

Next, we will repeat the steps for the two selected work groups. 
```{r, echo=FALSE}
reg1 = lm(as.numeric(mean_app_proc_time)~degree_centrality,data=examiner_joined_2wg)
reg2 = lm(as.numeric(mean_app_proc_time)~betweenness_centrality,data=examiner_joined_2wg)
reg3 = lm(as.numeric(mean_app_proc_time)~eigenvector_centrality,data=examiner_joined_2wg)
reg4 = lm(as.numeric(mean_app_proc_time)~closeness_centrality,data=examiner_joined_2wg)
reg5 = lm(as.numeric(mean_app_proc_time)~degree_centrality+betweenness_centrality+eigenvector_centrality+closeness_centrality,data=examiner_joined_2wg)
stargazer(reg1,reg2,reg3,reg4,reg5,type="text", title="Impacts of Centrality Measure Separately and Combined to Application Processing Time (2wg level)")
```

Unlike company level, all centrality measures except betweenness centrality reduce application process time for the two selected work groups. This finding is consistent across running regression separately and combined. 

Degree centrality, eigenvector centrality and closeness centrality have a negative relation to the mean application processing time. Adding one more unit of degree centrality subtract, on average, mean application processing time by 1.84 days and adding 0.1 unit of  eigenvector centrality and closeness centrality add 7.33e04 days and 1.14e08 respectively, if holding everything else equal. The higher these centrality scores, the faster the application processing. 

Only betweenness centrality have a positive relation to the mean application processing time. Adding 0.1 more unit in eigenvector centrality adds mean application processing time by 0.129 days, if holding everything else equal. This means having relationship with examiners who have the greatest influence over the flow of information would take longer processing time, potentially due to some bottleneck or centralized review needed within the work groups. 

Overall, the effect of centrality is greater for work groups 2450 and 2480 than in the entire USPTO organization. This is potentially due to the nature of applications that require more communications, collaborations and advice seeking in specific domain subjects. This is in line with the work group shortlisting approach taken in our methodology. 

#### 3.1.2 Impacts of examiners' gender
Now that we have a general idea on the relationship of centrality measures to application processing, we can overlay different dimensions on top and evaluate if there is different level of impacts. We will focus on the 2 work groups 1780 and 1770 shortlisted and begin with examiner's gender in t1 period. 

```{r, echo=FALSE}
# male
examiner_joined_2wg_m <- examiner_joined_2wg %>%
  filter(gender == "male")

reg1 = lm(as.numeric(mean_app_proc_time)~degree_centrality,data=examiner_joined_2wg_m)
reg2 = lm(as.numeric(mean_app_proc_time)~betweenness_centrality,data=examiner_joined_2wg_m)
reg3 = lm(as.numeric(mean_app_proc_time)~eigenvector_centrality,data=examiner_joined_2wg_m)
reg4 = lm(as.numeric(mean_app_proc_time)~closeness_centrality,data=examiner_joined_2wg_m)
reg5 = lm(as.numeric(mean_app_proc_time)~degree_centrality+betweenness_centrality+eigenvector_centrality+closeness_centrality,data=examiner_joined_2wg_m)
stargazer(reg1,reg2,reg3,reg4,reg5,type="text", title="Impacts of Centrality Measure Separately and Combined to Application Processing Time (2wg level, male)")
```

```{r, echo=FALSE}
# female
examiner_joined_2wg_f <- examiner_joined_2wg %>%
  filter(gender == "female")

reg1 = lm(as.numeric(mean_app_proc_time)~degree_centrality,data=examiner_joined_2wg_f)
reg2 = lm(as.numeric(mean_app_proc_time)~betweenness_centrality,data=examiner_joined_2wg_f)
reg3 = lm(as.numeric(mean_app_proc_time)~eigenvector_centrality,data=examiner_joined_2wg_f)
reg4 = lm(as.numeric(mean_app_proc_time)~closeness_centrality,data=examiner_joined_2wg_f)
reg5 = lm(as.numeric(mean_app_proc_time)~degree_centrality+betweenness_centrality+eigenvector_centrality+closeness_centrality,data=examiner_joined_2wg_f)
stargazer(reg1,reg2,reg3,reg4,reg5,type="text", title="Impacts of Centrality Measure Separately and Combined to Application Processing Time (2wg level, female)")
```


It is interesting to observe that the degree centrality effect in reducing application processing time is more in males (adding one more unit subtracts application processing time by 1.89 days) than females (subtracts 1.49 days) and the betweenness centrality effect adds application processing for male but reduces application processing time for female. This shows the different strengths and preferences on how to get applications processed by gender. Male examiners are more good at building cohesive network that all examiners know each other well, while female examiners are more good at bridging network that they have close examiners in different groups who don't know each other well. Also, bridging network seems to be more effective in reducing application processing time for female examiners while it could be counter-productive for male. 

To better understand potential reasons and control for other characteristics of examiner that might influence the relationship, let's take a look at the distribution of gender on company level and work group level.

```{r, echo=FALSE, warning=FALSE}
library(ggplot2)
library(scales)  
library(gridExtra)

plot1 <- ggplot(examiner_joined, aes(gender)) + 
          geom_bar(aes(y = (..count..)/sum(..count..))) + 
          scale_y_continuous(labels=scales::percent) +
          ylab("Relative Frequencies") + ylim(0,1) +
          ggtitle("Gender distribution for USPTO")
plot2 <- ggplot(examiner_joined_2wg, aes(gender)) + 
          geom_bar(aes(y = (..count..)/sum(..count..))) + 
          scale_y_continuous(labels=scales::percent) +
          ylab("Relative Frequencies") + ylim(0,1) +
          ggtitle("Gender distribution for 2wg")
plot3 <- ggplot(examiner_joined_1780, aes(gender)) + 
          geom_bar(aes(y = (..count..)/sum(..count..))) + 
          scale_y_continuous(labels=scales::percent) +
          ylab("Relative Frequencies") + ylim(0,1) +
          ggtitle("Gender distribution for 1780")
plot4 <- ggplot(examiner_joined_1770, aes(gender)) + 
          geom_bar(aes(y = (..count..)/sum(..count..))) + 
          scale_y_continuous(labels=scales::percent) +
          ylab("Relative Frequencies") + ylim(0,1) +
          ggtitle("Gender distribution for 1770")
grid.arrange(plot1,plot2,plot3,plot4,ncol=2, widths=c(1,1))
```

It is observed that there is systematic bias in gender for USPTO and selected work groups that there are more male examiners than female. Among the 2 selected work group, 1780 has better gender balance and 1770 has worse gender balance when compared to company-level. Gender shall be taken into consideration in the formulation of regression overall. 

Let's repeat running the regression with interaction terms of gender and centrality. This time we're going to take a more micro-view per each of the selected work groups as their gender distributions are so different. 
```{r, echo=FALSE}
# 1780, gender x centrality
reg1 = lm(as.numeric(mean_app_proc_time)~degree_centrality+as.factor(gender)+degree_centrality*as.factor(gender),data=examiner_joined_1780)
reg2 = lm(as.numeric(mean_app_proc_time)~betweenness_centrality+as.factor(gender)+betweenness_centrality*as.factor(gender),data=examiner_joined_1780)
reg3 = lm(as.numeric(mean_app_proc_time)~eigenvector_centrality+as.factor(gender)+eigenvector_centrality*as.factor(gender),data=examiner_joined_1780)
reg4 = lm(as.numeric(mean_app_proc_time)~closeness_centrality+as.factor(gender)+closeness_centrality*as.factor(gender),data=examiner_joined_1780)
reg5 = lm(as.numeric(mean_app_proc_time)~degree_centrality+betweenness_centrality+eigenvector_centrality+closeness_centrality+as.factor(gender)+degree_centrality*as.factor(gender)+betweenness_centrality*as.factor(gender)+eigenvector_centrality*as.factor(gender)+closeness_centrality*as.factor(gender),data=examiner_joined_1780)
stargazer(reg1,reg2,reg3,reg4,reg5,type="text", title="Impacts of Centrality Measure Separately and Combined to Application Processing Time (1780 wg level, centrality x gender)")
```

Looking into work group 1780 which has better gender balance, the effect of different centrality measures to mean application time separately in the 1st to 4th regression is consistent with combining them together for evaluation in the 5th regression. Increase in any of the four centrality measures can reduce the application processing time. 
Being a male examiner subtracts, on average, mean application processing time by 1693 days, if holding everything else equal. 
The interaction terms of gender x centrality has shown that being a male examiner, the effect of degree centrality is negative and the effect of betweenness centrality is positive in relation to the mean application processing time - this supports the robustness of our above finding per gender that male examiners are more good at building cohesive network while female examiners are more good at bridging network. 

```{r, echo=FALSE}
# 1770, gender x centrality
reg1 = lm(as.numeric(mean_app_proc_time)~degree_centrality+as.factor(gender)+degree_centrality*as.factor(gender),data=examiner_joined_1770)
reg2 = lm(as.numeric(mean_app_proc_time)~betweenness_centrality+as.factor(gender)+betweenness_centrality*as.factor(gender),data=examiner_joined_1770)
reg3 = lm(as.numeric(mean_app_proc_time)~eigenvector_centrality+as.factor(gender)+eigenvector_centrality*as.factor(gender),data=examiner_joined_1770)
reg4 = lm(as.numeric(mean_app_proc_time)~closeness_centrality+as.factor(gender)+closeness_centrality*as.factor(gender),data=examiner_joined_1770)
reg5 = lm(as.numeric(mean_app_proc_time)~degree_centrality+betweenness_centrality+eigenvector_centrality+closeness_centrality+as.factor(gender)+degree_centrality*as.factor(gender)+betweenness_centrality*as.factor(gender)+eigenvector_centrality*as.factor(gender)+closeness_centrality*as.factor(gender),data=examiner_joined_1770)
stargazer(reg1,reg2,reg3,reg4,reg5,type="text", title="Impacts of Centrality Measure Separately and Combined to Application Processing Time (1770 wg level, centrality x gender)")
```

Looking into work group 1770 which has more severe gender imbalance towards male, the overall findings are similar to work group 1780 - the effect of different centrality measures to mean application time separately in the 1st to 4th regression is consistent with combining them together for evaluation in the 5th regression. Increase in any of the four centrality measures can reduce the application processing time. 
Being a male examiner subtracts, on average, mean application processing time by 274 days, if holding everything else equal. 

However, the interaction terms of gender x centrality has shown that being a male examiner, the effect of degree centrality is positive and the effect of betweenness centrality is negative in relation to the mean application processing time - this can potentially be explained by the hypothesis that even male examiners are more good at building cohesive network it may be a cons than pros in a male dominating work group, and that even female examiners are more good at bridging network they might not perform as well when they are the minority. These warrant further analysis beyond application processing time on how the gender distributions impact examiners' networking among different work groups. 

#### 3.1.3 Impacts of examiners' seniority
We are interested in how examiners' seniority influences the application process time. Here, we focus on the seniority state of examiners in t1 period. 

Firstly, We fit a linear regression between the mean application processing time and the centralities for all junior examiners. 
```{r, include=FALSE}
examiner_joined_2wg$t1_state <- as.factor(examiner_joined_2wg$t1_state)

examiner_joined_2wg_junior <- examiner_joined_2wg %>%
  filter(t1_state == "Junior")

seniority_junior_reg = lm(as.numeric(mean_app_proc_time) ~ degree_centrality + betweenness_centrality + eigenvector_centrality + closeness_centrality, data=examiner_joined_2wg_junior)

summary(seniority_junior_reg)
```
Based on the summary, for junior examiners, an increase of 1 unit of degree centrality, betweenness centrality, and eigen vector centrality, means a decrease of process time by 2.76 days, 8.63 days and 1.32e+98 days respectively. Only the closeness centrality has positive relationship with the processing time, which means the higher the closeness centrality, the longer the processing time. Unfortunately, all of the coefficients are not statistically significant.


Then, we fit the similar linear regression based on data points from senior examiners to see if seniority impose some impacts. 
```{r, include=FALSE}
examiner_joined_2wg_senior <- examiner_joined_2wg %>%
  filter(t1_state == "Senior")

seniority_senior_reg = lm(as.numeric(mean_app_proc_time) ~ degree_centrality + betweenness_centrality + eigenvector_centrality + closeness_centrality, data=examiner_joined_2wg_senior)

stargazer(seniority_junior_reg,seniority_senior_reg,type="text", title="Impacts of Seniority to Application Processing Time")
```
Different from the juniors, for senior examiners, an increase of 1 unit of degree centrality and eigen vector centrality means a decrease of process time by 2.86 days (larger than the amount of change for juniors) and 1.33e+08 days(smaller than the amount of change for juniors) respectively. Contrastingly, an increase of 1 unit of betweenness centrality and closeness centrality causes an increase of process time by 6.14 days and 4.15 days respectively. Similarly, all of the coefficients are not statistically significant.


In the next model, we include seniority as interaction term for each of the centrality predictors. 
```{r, echo=FALSE}
seniority_reg = lm(as.numeric(mean_app_proc_time) ~ t1_state*degree_centrality + t1_state*betweenness_centrality + t1_state*eigenvector_centrality + t1_state*closeness_centrality, data=examiner_joined_2wg)

stargazer(seniority_junior_reg,seniority_senior_reg, seniority_reg,type="text", title="Impacts of Seniority to Application Processing Time")
```
According to this model, a junior examiner usually process applications 9.60 days longer than a senior examiner. Also, the same as the previous junior model, an increase in any of the centrality predictor results in shorter processing time for junior examiners, except for the closeness centrality. If the examiner is a senior, an increase in closeness centrality results in a shortening of 3.43e08 days compared to juniors; and increase in degree centrality or betweenness centrality or eigen vector centrality incurs a elongation of 1.38 days, 9.17 days or 1.11e09 days compared to juniors. 


#### 3.1.3 Impacts of examiners' race and ethnicity
```{r, include=FALSE}
# asian
examiner_joined_2wg_asian <- examiner_joined_2wg %>% filter(race == "asian")

asian_reg = lm(as.numeric(mean_app_proc_time) ~ degree_centrality + betweenness_centrality + eigenvector_centrality + closeness_centrality, data=examiner_joined_2wg_asian)

summary(asian_reg)
```


```{r, include=FALSE}
# black
examiner_joined_2wg_black <- examiner_joined_2wg %>% filter(race == "black")

black_reg = lm(as.numeric(mean_app_proc_time) ~ degree_centrality + betweenness_centrality + eigenvector_centrality + closeness_centrality, data=examiner_joined_2wg_black)

summary(black_reg)
```

```{r, include=FALSE}
# hispanic
examiner_joined_2wg_hispanic <- examiner_joined_2wg %>% filter(race == "hispanic")

hispanic_reg = lm(as.numeric(mean_app_proc_time) ~ degree_centrality + betweenness_centrality + eigenvector_centrality + closeness_centrality, data=examiner_joined_2wg_hispanic)

summary(hispanic_reg)
```

```{r, include=FALSE}
# other
examiner_joined_2wg_other <- examiner_joined_2wg %>% filter(race == "other")

other_reg = lm(as.numeric(mean_app_proc_time) ~ degree_centrality + betweenness_centrality + eigenvector_centrality + closeness_centrality, data=examiner_joined_2wg_other)

summary(other_reg)
```

```{r, include=FALSE}
# white
examiner_joined_2wg_white <- examiner_joined_2wg %>% filter(race == "white")

white_reg = lm(as.numeric(mean_app_proc_time) ~ degree_centrality + betweenness_centrality + eigenvector_centrality + closeness_centrality, data=examiner_joined_2wg_white)

summary(white_reg)
```

```{r, echo=FALSE}
stargazer(asian_reg,black_reg,hispanic_reg,other_reg,white_reg,type="text", title="Impacts of Race to Application Processing Time")
```

Adding one more unit in degree centrality subtracts, on average, an asian’s mean application processing time by 5.4 days, if holding everything else equal. Adding one more unit in betweenness centrality subtracts, on average, an asian’s mean application processing time by 4.7 days, if holding everything else equal.

Adding one more unit in degree centrality subtracts, on average, a white’s mean application processing time by 2.4 days, if holding everything else equal. Adding one more unit in betweenness centrality adds, on average, a white’s mean application processing time by 0.03 days, if holding everything else equal.

```{r, echo=FALSE}
aggregate(data.frame(count = examiner_joined_2wg$race), list(value = examiner_joined_2wg$race), length)
```

This distribution explains why the regressions made for races with not enough datapoints didn't work.

To better understand potential reasons and control for other characteristics of examiner that might influence the relationship, let’s take a look at the distribution of race for the 2 work groups combined.

```{r, include=FALSE}
library(ggplot2)
library(scales) 
library(gridExtra)
```

```{r, echo=FALSE, warning=FALSE}
plot1 <- ggplot(examiner_joined_2wg, aes(race)) + 
          geom_bar(aes(y = (..count..)/sum(..count..))) + 
          scale_y_continuous(labels=scales::percent) +
          ylab("Relative Frequencies") +
          ggtitle("Race distribution for USPTO")

plot2 <- ggplot(examiner_joined_2wg, aes(race, mean_app_proc_time)) + 
          geom_bar(posititon="dodge", stat="summary", fun="mean") + 
          ylab("Mean App Proc Time (Days)") +
          ggtitle("App Proc Time for USPTO")

grid.arrange(plot1,plot2,ncol=2, widths=c(1,1))
```

Let's consider race at the both workgroups level now.

```{r, echo=FALSE}
race_reg = lm(as.numeric(mean_app_proc_time) ~ degree_centrality + betweenness_centrality + eigenvector_centrality + closeness_centrality + as.factor(race) + as.factor(race)*degree_centrality + as.factor(race)*betweenness_centrality, data=examiner_joined_2wg)

summary(race_reg)
```

Adding one more unit in degree centrality subtracts, on average, mean application processing time by 2.8 days, if holding everything else equal. Adding one more unit in betweenness centrality subtracts, on average, mean application processing time by 1 days, if holding everything else equal. Being a black examiner adds, on average, mean application processing time by 185 days, if holding everything else equal. The interaction terms of race x centrality has shown that being a white examiner, the effect of degree centrality is positive and the effect of betweenness centrality is positive in relation to the mean application processing time.


# 4. Conclusions and Recommendations

To USPTO, it is important to understand the fundamental reasons of gender imbalance within the organization and why male examiners in general takes 274 less days to process applications in selected work groups. With centrality, we can conclude that a cohesive network around an examiner is important to the efficiency of processing patent applications, and implies that application processing is more an non-divergent organizational change than a divergent change. From gender impact analysis, we are also aware of the strengths and working style of females and males that male examiners are more good at building cohesive network that all examiners know each other well, while female examiners are more good at bridging network that they have close examiners in different groups who don't know each other well - this applies when there is better gender balance, and less of a benefit under gender imbalance.

With that being said, since the main duty of examiners are processing applications, it makes sense that naturally more male examiners with network building capabilities are acquired to perform the day-to-day task. However, from a gender equity perspective, it is more beneficial to strike a gender balance to promote organization diversity & inclusion. To USPTO business, there could be other transformation projects (divergent changes perhaps) which might require involvement of examiners on top of business as usual. 

Considering the impact of seniority to the mean application processing time, we found that junior examiners take 9.60 days longer to process applications than senior and that for senior, having higher centrality score actually hinders their speed in processing the patent applications. This could be explained by the greater responsibility they take being involved in cohesive and bridging networks under the context of an advice net. This is supported by the fact that there more senior being consulted than seeking advice in the directed network, and pointing towards the need to review workload per seniority level and the coaching and mentoring approach to enhance operational efficiency. 

Imbalance of examiners' race and ethnicity is also seen which lack of Black and Hispanic data points are too few for us to draw useful conclusion on the company level. Under similar limitations, a closer look into the selected work groups at least shed us some light on potential racial inequality issue that being a black examiner takes 185 days more in mean processing time while networking for white examiners helps reducing mean application processing time. Further study on how the racial distribution looks like against the demographic of US might help evaluation of inclusion and diversity policy and formulation of targeted internal training and team building to break the obstacles to bond and perform efficiently.  

Last but not least, analysis on specific work group level and individual level are strongly recommended to evaluate examiner performance more fairly in quarterly/yearly reviews. Different metrics apart from application processing time / efficiency in handling applications shall be included, such as the quality of applications processing, tenure and promotion, etc. In this way, we can fit the context of organizational network analysis into the bigger picture of human resource strategies.  
